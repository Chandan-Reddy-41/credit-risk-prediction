{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35eff1d-c763-4d8a-8771-16d3db5c0718",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5eb25-bf68-4e41-8601-947f539b069d",
   "metadata": {},
   "source": [
    "## 1. Introduction to Pandas\n",
    "Pandas is a fast, powerful, flexible, and easy-to-use open-source data analysis and manipulation library, built on top of the Python programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b79ce7-b5a7-446c-98e2-6134b5195816",
   "metadata": {},
   "source": [
    "**Why Pandas?**\n",
    "\n",
    "While NumPy is great for homogeneous numerical data, Pandas offers labeled data structures, making it far superior for handling the messy, real-world data typically found in spreadsheets or databases.\n",
    "\n",
    "**Labeled Data**: It allows you to refer to data by meaningful column names (like 'Date', 'Sales', 'Product ID') and row names (indices).\n",
    "\n",
    "**Missing Data Handling**: Pandas handles missing data (represented as **NaN**) easily.\n",
    "\n",
    "**Data Alignment**: Operations automatically align data by labels, preventing calculation errors when working with non-aligned datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbfdb39-e44e-4d5d-b5c8-9a6f749ee938",
   "metadata": {},
   "source": [
    "Pandas introduces two Core primary Data structures:\n",
    "\n",
    "**Series**: A 1D labeled array (think of a single column of data). It is built on a 1D NumPy array.\n",
    "\n",
    "**DataFrame**: A 2D labeled data structure with columns of potentially different types (think of an entire spreadsheet or SQL table). It is built on 2D NumPy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4dc36d-f61d-4b94-8c30-e159a66275cb",
   "metadata": {},
   "source": [
    "## 2. The Pandas Series (1D Labeled Array)\n",
    "A **Series** is like a 1D NumPy array but with an explicit index associated with each element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181afae-0be5-4fda-8e22-0b22fe2a708d",
   "metadata": {},
   "source": [
    "### 2.1 Creating a **Series** We start by importing Pandas, conventionally aliased as **pd**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd3c8efa-7aa4-4193-8fc6-d0b353f602ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # importing pandas as pd\n",
    "import numpy as np # importing numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e36a3f3-32bb-468a-8dae-f24569fdf54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy array: [10 15 20 25 30]\n",
      "Pandas series: \n",
      "0    10\n",
      "1    15\n",
      "2    20\n",
      "3    25\n",
      "4    30\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# creating numpy array\n",
    "numpy_array=np.array([10,15,20,25,30])\n",
    "print(f\"Numpy array: {numpy_array}\")\n",
    "\n",
    "# creating pandas series with numpy array\n",
    "pandas_series=pd.Series(numpy_array)\n",
    "print(\"Pandas series: \")\n",
    "print(pandas_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02dfb1-22ee-4f97-b7e1-fa06db213df3",
   "metadata": {},
   "source": [
    "**Explanation**: Notice the two columns. The left column (**0, 1, 2, 3,4**) is the Index (or label). The right column is the Value (the data itself)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8565cde1-b8d4-4cc0-86d3-2db6ece899ed",
   "metadata": {},
   "source": [
    "### 2.2 Adding Custom Labels (Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfbddab9-28d3-47a9-9f2d-298f9998044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week sales series: \n",
      "sun    11\n",
      "mon    21\n",
      "tue    31\n",
      "wed    41\n",
      "thu    51\n",
      "fri    61\n",
      "sat    71\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# creating series with custom lables (index)\n",
    "sales_data=[11,21,31,41,51,61,71]\n",
    "sales_lables=['sun','mon','tue','wed','thu','fri','sat']\n",
    "\n",
    "\n",
    "sales_series=pd.Series(data=sales_data, index=sales_lables)\n",
    "print(\"Week sales series: \")\n",
    "print(sales_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909fbdf9-00d4-4812-94b7-13219f787739",
   "metadata": {},
   "source": [
    "**Key Takeaway**: The data is stored efficiently in a NumPy array, but the labels make it easy to understand and access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b23546e-0271-41d8-9461-98c9ced2fb2d",
   "metadata": {},
   "source": [
    "### 2.3 Series Indexing and Operations\n",
    "Indexing works just like NumPy arrays, but you can also use the custom labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e46ba5-d4fc-4ae5-bab6-83903a5d5ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed sales: 41\n",
      "Mon sales: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chand\\AppData\\Local\\Temp\\ipykernel_21376\\2888456798.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mon_sales=sales_series[1]\n"
     ]
    }
   ],
   "source": [
    "# Accessing data using label indexing (like a dictionary key)\n",
    "wed_sales=sales_series['wed']\n",
    "print(f\"Wed sales: {wed_sales}\")\n",
    "# Accessing data using position indexing (like a list index)\n",
    "mon_sales=sales_series[1]\n",
    "print(f\"Mon sales: {mon_sales}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72ec8b55-fb79-4c45-9f28-9ebb57fe3a70",
   "metadata": {},
   "source": [
    "## 3. The Pandas $\\texttt{DataFrame}$ (2D Labeled Structure)\n",
    "\n",
    "A **DataFrame** is the central Pandas object. It is a collection of **Series** objects that share the same index, effectively forming a table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164cc8da-13be-4460-9ae0-bcbaac6376da",
   "metadata": {},
   "source": [
    "### 3.1 Creating a Data Frame\n",
    "The most common way to create a **DataFrame** manually is from a dictionary of Python lists or NumPy arrays. The keys of the dictionary become the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b5fffa-b601-4e93-81de-58f3eda4263d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "students DataFrame: \n",
      "   Exam Score  Attendance Result\n",
      "0          85          95   Pass\n",
      "1          92          98   Pass\n",
      "2          78          85   Fail\n",
      "3          95         100   Pass\n",
      "4          88          92   Pass\n"
     ]
    }
   ],
   "source": [
    "# Data structured as a dictionary (Keys are Column Headers)\n",
    "students_data = {\n",
    "    'Exam Score': [85, 92, 78, 95, 88],\n",
    "    'Attendance': [95, 98, 85, 100, 92],\n",
    "    'Result': ['Pass', 'Pass', 'Fail', 'Pass', 'Pass']\n",
    "}\n",
    "\n",
    "# creating a DataFrame with dict\n",
    "df_students=pd.DataFrame(students_data)\n",
    "print(\"students DataFrame: \")\n",
    "print(df_students)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289b2cd-336d-4e55-b44a-3abf16031dd7",
   "metadata": {},
   "source": [
    "### 3.2 Creating Custom Row Lables (Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86f43b3-d80b-475e-8eca-cfd8f787e55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students DataFrame: \n",
      "      Exam Score  Attendance Result\n",
      "S101          85          95   Pass\n",
      "S102          92          98   Pass\n",
      "S103          78          85   Fail\n",
      "S104          95         100   Pass\n",
      "S105          88          92   Pass\n"
     ]
    }
   ],
   "source": [
    "# Set custom row labels (Student IDs)\n",
    "students_ids = ['S101', 'S102', 'S103', 'S104', 'S105']\n",
    "\n",
    "df_students=pd.DataFrame(students_data, index=students_ids)\n",
    "print(\"Students DataFrame: \")\n",
    "print(df_students)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d7debe-a9d8-4f88-bc6e-0e32b4270fe4",
   "metadata": {},
   "source": [
    "## 4. Selection and Filtering with .loc and .iloc\n",
    "In Pandas, direct bracket indexing (like df **['Column']**) is used mainly for selecting columns. For selecting both rows and columns simultaneously, especially with filtering, we use special accessors.\n",
    "\n",
    "Syntax: **df.loc[row\\_label\\_selection, column\\_label\\_selection]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df00ff3-9d6e-417b-b807-5a97b0436edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Stock  Price    Sun    Mon     Tue    Wed     Thu    Fri     Sat\n",
      "Phone       105  21000  45000  85000   55000  65000   65000  35000   95000\n",
      "Laptop      110  65000  32000  92000   98000  62000   68000  82000   68000\n",
      "Mouse       115   1500  78000  68000   85000  48000   65000  88000   95000\n",
      "Keyboard    120   2000  95000  75000  100000  35000  100000  85000  100000\n",
      "Speaker     125   5000  88000  98000   92000  58000   72000  58000   92000\n"
     ]
    }
   ],
   "source": [
    "# Data structured as a dictionary (Keys are Column Headers)\n",
    "week_sales_data = {\n",
    "    'Stock': [105,110,115,120,125],\n",
    "    'Price': [21000,65000,1500,2000,5000],\n",
    "    'Sun': [45000, 32000, 78000, 95000, 88000],\n",
    "    'Mon': [85000, 92000, 68000, 75000, 98000],\n",
    "    'Tue': [55000, 98000, 85000, 100000, 92000],\n",
    "    'Wed': [65000, 62000, 48000, 35000, 58000],\n",
    "    'Thu': [65000, 68000, 65000, 100000, 72000],\n",
    "    'Fri': [35000, 82000, 88000, 85000, 58000],\n",
    "    'Sat': [95000, 68000, 95000, 100000, 92000]\n",
    "}\n",
    "Product=['Phone','Laptop','Mouse','Keyboard','Speaker']\n",
    "# create Dataframe with Dict\n",
    "df_sales= pd.DataFrame(week_sales_data, index=Product)\n",
    "print(df_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce9c6bf-5d6a-4665-bd22-d31429254ca0",
   "metadata": {},
   "source": [
    "#### Adding the Total_sales Column (Vectorised Operation)\n",
    "\n",
    "This is where the NumPy foundation shines! You create a new column by multiplying two existing Series objects (columns). The operation is automatically broadcast element-wise across the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0675f18d-591d-468e-a81e-2566cf9a3d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Stock  Price    Sun    Mon     Tue    Wed     Thu    Fri     Sat  \\\n",
      "Phone       105  21000  45000  85000   55000  65000   65000  35000   95000   \n",
      "Laptop      110  65000  32000  92000   98000  62000   68000  82000   68000   \n",
      "Mouse       115   1500  78000  68000   85000  48000   65000  88000   95000   \n",
      "Keyboard    120   2000  95000  75000  100000  35000  100000  85000  100000   \n",
      "Speaker     125   5000  88000  98000   92000  58000   72000  58000   92000   \n",
      "\n",
      "          Total_value  \n",
      "Phone         2205000  \n",
      "Laptop        7150000  \n",
      "Mouse          172500  \n",
      "Keyboard       240000  \n",
      "Speaker        625000  \n"
     ]
    }
   ],
   "source": [
    "# creating a new column Total sales \n",
    "# Create the new column using element-wise multiplication (Broadcasting!)\n",
    "df_sales['Total_value']= df_sales['Stock'] * df_sales['Price']\n",
    "print(df_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde73d9-b6f2-4f46-a033-4e4a4644439c",
   "metadata": {},
   "source": [
    "### 4.1 Accessor .loc (Label-based Indexing)\n",
    "The **.loc** accessor uses labels (index names and column names) for selection.\n",
    "\n",
    "**Syntax**: **df.loc[row\\_label\\_selection, column\\_label\\_selection]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6100359b-6da2-4587-a7d1-9d5f8288aa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Keyboard Stock (loc): 120\n",
      "\n",
      "2. Laptop & Monitor Data (loc):\n",
      "         Stock  Price\n",
      "Laptop     110  65000\n",
      "Speaker    125   5000\n",
      "\n",
      "3. Data from Speaker through Mouse (loc slice):\n",
      "          Stock  Price    Sun    Mon     Tue    Wed     Thu    Fri     Sat  \\\n",
      "Mouse       115   1500  78000  68000   85000  48000   65000  88000   95000   \n",
      "Keyboard    120   2000  95000  75000  100000  35000  100000  85000  100000   \n",
      "Speaker     125   5000  88000  98000   92000  58000   72000  58000   92000   \n",
      "\n",
      "          Total_value  \n",
      "Mouse          172500  \n",
      "Keyboard       240000  \n",
      "Speaker        625000  \n"
     ]
    }
   ],
   "source": [
    "# Example 1: Selecting a Single Row and Single Column by Label\n",
    "# Access the 'Stock' value for 'Keyboard'\n",
    "keyboard_stock = df_sales.loc['Keyboard', 'Stock']\n",
    "print(f\"1. Keyboard Stock (loc): {keyboard_stock}\")\n",
    "\n",
    "# Example 2: Selecting Multiple Rows and Multiple Columns by Label\n",
    "# Access 'Stock' and 'Price' for 'Laptop' and 'Speaker'\n",
    "laptops_and_Speaker_data = df_sales.loc[['Laptop', 'Speaker'], ['Stock', 'Price']]\n",
    "print(f\"\\n2. Laptop & Monitor Data (loc):\\n{laptops_and_Speaker_data}\")\n",
    "\n",
    "# Example 3: Slicing by Label (inclusive!)\n",
    "# Get all data from 'Speaker' through 'Mouse' (inclusive for .loc)\n",
    "Speaker_to_mouse = df_sales.loc['Mouse':'Speaker', :] \n",
    "print(f\"\\n3. Data from Speaker through Mouse (loc slice):\\n{Speaker_to_mouse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879770f-d4da-4108-a2e2-65dba10d2c5e",
   "metadata": {},
   "source": [
    "### 4.2 Accessor .iloc (Integer-based Indexing)\n",
    "The **.iloc** accessor uses integer positions (starting from 0) for selection, just like NumPy array indexing. This is useful when you don't care about the labels, only the position.\n",
    "\n",
    "Syntax: **df.iloc[row\\_position\\_selection, column\\_position\\_selection]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b02ad51a-2351-4813-8aea-96aec4141503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Keyboard Total Value (iloc): 68000\n",
      "\n",
      "5. First two items, Units and Price (iloc slice):\n",
      "        Price    Sun\n",
      "Phone   21000  45000\n",
      "Laptop  65000  32000\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Selecting a Single Cell by Position\n",
    "# Access the 'Price' (column index 3) for the 3rd item ('Keyboard' - row index 3)\n",
    "keyboard_price_iloc = df_sales.iloc[2, 3] \n",
    "print(f\"\\n4. Keyboard Total Value (iloc): {keyboard_price_iloc}\")\n",
    "\n",
    "# Example 5: Selecting a Subset using Slicing (exclusive!)\n",
    "# Get the first two rows (0 and 1) and the first two data columns (1 and 2: Units and Price)\n",
    "subset_iloc = df_sales.iloc[0:2, 1:3]\n",
    "print(f\"\\n5. First two items, Units and Price (iloc slice):\\n{subset_iloc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94861a4f-8e34-48f9-b77d-974a6a8a3e0f",
   "metadata": {},
   "source": [
    "### 4.3 Conditional Filtering (Boolean Indexing in Pandas)\n",
    "This is the most common use case and directly leverages your knowledge of NumPy's Boolean Indexing!\n",
    "\n",
    "Concept: You create a Boolean Series (a column of **True/False** values) by applying a condition to a column. You then pass that Boolean Series to **.loc** to filter the rows.\n",
    "\n",
    "Syntax: **df.loc[df['Column'] operator value]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19780bdb-f707-42a0-844b-efef0eeb2c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Price Filter Mask (Boolean Series):\n",
      "Phone        True\n",
      "Laptop       True\n",
      "Mouse       False\n",
      "Keyboard    False\n",
      "Speaker     False\n",
      "Name: Price, dtype: bool\n",
      "\n",
      "6. Items with Price > $10000 (Filtered):\n",
      "        Stock  Price    Sun    Mon    Tue    Wed    Thu    Fri    Sat  \\\n",
      "Phone     105  21000  45000  85000  55000  65000  65000  35000  95000   \n",
      "Laptop    110  65000  32000  92000  98000  62000  68000  82000  68000   \n",
      "\n",
      "        Total_value  \n",
      "Phone       2205000  \n",
      "Laptop      7150000  \n"
     ]
    }
   ],
   "source": [
    "# Find all items where the Price is greater than $10000\n",
    "# Step 1: Create the Boolean Series (the mask)\n",
    "price_filter_mask = df_sales['Price'] > 10000\n",
    "print(f\"\\nPrice Filter Mask (Boolean Series):\\n{price_filter_mask}\")\n",
    "\n",
    "# Step 2: Use .loc to apply the mask to the rows\n",
    "expensive_items = df_sales.loc[price_filter_mask]\n",
    "\n",
    "# Or in one line:\n",
    "expensive_items = df_sales.loc[df_sales['Price'] > 10000]\n",
    "\n",
    "print(f\"\\n6. Items with Price > $10000 (Filtered):\\n{expensive_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6664dc8-a5f7-49eb-b5d8-7f0799ba1a39",
   "metadata": {},
   "source": [
    "## 5. Grouping and Summarizing Data (Groupby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6f318-4c93-4a46-a4a8-7304b9567cdb",
   "metadata": {},
   "source": [
    "The next major topic is the groupby() method, which is the most essential tool for exploratory data analysis (EDA). It allows you to split your data into groups based on some criterion, apply a function (like **mean**, **sum**, **count**) to each group, and then combine the results.\n",
    "\n",
    "This is often referred to as the Split-Apply-Combine strategy.\n",
    "\n",
    "Concept: Split-Apply-Combine\n",
    "\n",
    "**Split**: The DataFrame is split into groups based on the unique values in a specified column (e.g., grouping sales data by 'Region').\n",
    "\n",
    "**Apply**: An aggregation function (like **.sum()** or **.mean()**) is applied to the columns of each individual group.\n",
    "\n",
    "**Combine**: The results are combined into a new DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d26b5-420c-4812-8d42-ac3a33787048",
   "metadata": {},
   "source": [
    "Code Example: Grouping Data\n",
    "Let's use a new, slightly larger DataFrame representing sales data across regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c488065f-1268-4db8-8453-3e8715492a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sales Data:\n",
      "  Region Salesperson  Revenue  Items_Sold\n",
      "0  North       Alice     1500          10\n",
      "1  South         Bob     2200          15\n",
      "2  North       Alice     1800          12\n",
      "3   West     Charlie     3000          20\n",
      "4  South         Bob     1900          14\n",
      "5   West       David     2500          18\n",
      "\n",
      "Regional Sales Totals (Sum):\n",
      "         Salesperson  Revenue  Items_Sold\n",
      "Region                                   \n",
      "North     AliceAlice     3300          22\n",
      "South         BobBob     4100          29\n",
      "West    CharlieDavid     5500          38\n"
     ]
    }
   ],
   "source": [
    "sales_data = {\n",
    "    'Region': ['North', 'South', 'North', 'West', 'South', 'West'],\n",
    "    'Salesperson': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob', 'David'],\n",
    "    'Revenue': [1500, 2200, 1800, 3000, 1900, 2500],\n",
    "    'Items_Sold': [10, 15, 12, 20, 14, 18]\n",
    "}\n",
    "df_sales = pd.DataFrame(sales_data)\n",
    "\n",
    "print(\"Original Sales Data:\")\n",
    "print(df_sales)\n",
    "\n",
    "# 1. Group the data by the 'Region' column\n",
    "by_region = df_sales.groupby('Region')\n",
    "\n",
    "# 2. Apply an aggregation function (e.g., calculate the SUM of Revenue and Items_Sold for each region)\n",
    "# Only numerical columns are summed.\n",
    "regional_totals = by_region.sum() \n",
    "\n",
    "print(f\"\\nRegional Sales Totals (Sum):\\n{regional_totals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba33c41-bcf8-49fe-9862-e7ee74739b84",
   "metadata": {},
   "source": [
    "#### Common Groupby Operations\n",
    "\n",
    "Aggregation Function | Description\n",
    "\n",
    "**.sum()** | Total of all values in the group.\n",
    "\n",
    "**.mean()** | Average of all values in the group.\n",
    "\n",
    "**.count()** | Number of non-NaN values in the group.\n",
    "\n",
    "**.max()/.min()** | Maximum/Minimum value in the group.\n",
    "\n",
    "**.describe()** | \"Returns multiple summary statistics (count, mean, std, min, max, quartiles).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c828fbdf-c537-47ef-b8fb-444cbab69056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original DataFrame ---\n",
      "  Region Month  Sales  Returns\n",
      "0   East   Jan    100      5.0\n",
      "1   East   Feb    150     10.0\n",
      "2   West   Jan    200      NaN\n",
      "3   East   Mar    120      8.0\n",
      "4   West   Feb    180     12.0\n",
      "5   West   Mar    250     15.0\n",
      "6   East   Apr     90      NaN\n",
      "7   West   Apr    220     18.0\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Region': ['East', 'East', 'West', 'East', 'West', 'West', 'East', 'West'],\n",
    "    'Month': ['Jan', 'Feb', 'Jan', 'Mar', 'Feb', 'Mar', 'Apr', 'Apr'],\n",
    "    'Sales': [100, 150, 200, 120, 180, 250, 90, 220],\n",
    "    'Returns': [5, 10, np.nan, 8, 12, 15, np.nan, 18] # Note the NaN (missing) values\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"--- Original DataFrame ---\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b00de-1c5f-4a0c-8a56-413b0940ff36",
   "metadata": {},
   "source": [
    "#### 1. .sum() Total of all values in the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f981608-5a62-4667-95a5-e3459c103927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".sum()\n",
      "               Month  Sales  Returns\n",
      "Region                              \n",
      "East    JanFebMarApr    460     23.0\n",
      "West    JanFebMarApr    850     45.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total Sales and total Returns for each Region\n",
    "result_sum = df.groupby('Region').sum()\n",
    "print('.sum()')\n",
    "print(result_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c94d2b2-cd6d-4379-b09b-d8644236cb54",
   "metadata": {},
   "source": [
    "#### 2. .mean(): Average of 'Sales' and 'Returns' columns in the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ecb08d6-c8c5-49ff-bb2c-d8451fa1ef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".mean()\n",
      "        Sales    Returns\n",
      "Region                  \n",
      "East    115.0   7.666667\n",
      "West    212.5  15.000000\n"
     ]
    }
   ],
   "source": [
    "# Select ONLY the 'Sales' and 'Returns' columns for the mean calculation\n",
    "result_mean = df.groupby('Region')[['Sales', 'Returns']].mean()\n",
    "print('.mean()')\n",
    "print(result_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa241e-235c-49dd-b560-6402b33e2e82",
   "metadata": {},
   "source": [
    "#### 3. .count(): Number of non-NaN values in the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f430756-3efa-4fae-bbc4-32fd6e750055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".count()\n",
      "        Month  Sales  Returns\n",
      "Region                       \n",
      "East        4      4        3\n",
      "West        4      4        3\n"
     ]
    }
   ],
   "source": [
    "# Count the number of non-NaN entries (transactions) for each column\n",
    "result_count = df.groupby('Region').count()\n",
    "print('.count()')\n",
    "print(result_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2dc54-f863-4f7c-bcc5-f1e900da2227",
   "metadata": {},
   "source": [
    "#### 4. .max() / .min(): Maximum/Minimum value in the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d99de12a-8c5a-43c2-b8b0-057423844c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".max()\n",
      "       Month  Sales  Returns\n",
      "Region                      \n",
      "East     Mar    150     10.0\n",
      "West     Mar    250     18.0\n",
      ".min()\n",
      "       Month  Sales  Returns\n",
      "Region                      \n",
      "East     Apr     90      5.0\n",
      "West     Apr    180     12.0\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum value in each group\n",
    "result_max = df.groupby('Region').max()\n",
    "print(\".max()\")\n",
    "print(result_max)\n",
    "\n",
    "# Find the minimum value in each group\n",
    "result_min = df.groupby('Region').min()\n",
    "print(\".min()\")\n",
    "print(result_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f5b69a-647d-4751-a715-ca5a9aed4386",
   "metadata": {},
   "source": [
    "#### 5. .describe(): Multiple Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df04d628-bafc-4839-9157-7a40018bb2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".describe()\n",
      "        count   mean        std    min    25%    50%    75%    max\n",
      "Region                                                            \n",
      "East      4.0  115.0  26.457513   90.0   97.5  110.0  127.5  150.0\n",
      "West      4.0  212.5  29.860788  180.0  195.0  210.0  227.5  250.0\n"
     ]
    }
   ],
   "source": [
    "# Returns a summary table of statistics for each numerical column\n",
    "result_describe = df.groupby('Region').describe()\n",
    "print(\".describe()\")\n",
    "# Only printing the 'Sales' part for brevity, as the output is wide\n",
    "print(result_describe['Sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be03c7-c2db-4070-bbd1-e4a992083989",
   "metadata": {},
   "source": [
    "## 6. Handling Missing Data (NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e4d0cf-7428-400f-bf8d-1447d8f31337",
   "metadata": {},
   "source": [
    "The next major topic is dealing with missing values, often represented as **NaN** (Not a Number) in Pandas (which is a float value managed by NumPy). Real-world data is almost always messy, so this is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be6ece-a45b-4c85-8814-20dcd3d31f2a",
   "metadata": {},
   "source": [
    "### 6.1 Identifing missing data\n",
    "The methods **.isnull()** (or **.isna()**) and **.notnull()** (or **.notna()**) return Boolean DataFrames/Series indicating where data is missing (**True**) or present (**False**)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8595dee-8711-4981-96b5-9aed3c7b843e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Null Check:\n",
      "        A      B      C\n",
      "0  False  False  False\n",
      "1  False   True  False\n",
      "2   True  False  False\n",
      "3  False  False  False\n",
      "\n",
      "Total Missing Values per Column:\n",
      "A    1\n",
      "B    1\n",
      "C    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with missing data\n",
    "data_missing = {\n",
    "    'A': [1, 2, np.nan, 4],\n",
    "    'B': [5, np.nan, 7, 8],\n",
    "    'C': [9, 10, 11, 12]\n",
    "}\n",
    "df_nan = pd.DataFrame(data_missing)\n",
    "\n",
    "# Check where the values are missing\n",
    "print(\"Is Null Check:\\n\", df_nan.isnull())\n",
    "\n",
    "# Get a count of missing values per column (chaining methods)\n",
    "print(\"\\nTotal Missing Values per Column:\")\n",
    "print(df_nan.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547aba8b-c28d-4e89-9b71-2798192f68c1",
   "metadata": {},
   "source": [
    "### 6.2 Dropping Missing Data (.dropna())\n",
    "The simplest way to handle missing data is to remove the rows or columns that contain it.\n",
    "\n",
    "**Argument** | Description\n",
    "\n",
    "**axis=0** | Drop rows (default).\n",
    "\n",
    "**axis=1** | Drop columns.\n",
    "\n",
    "**how='any'** | Drop the row/column if any value is NaN (default).\n",
    "\n",
    "**how='all'** | Drop the row/column only if all values are NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cdf0651-befe-43af-b53c-4c123f25ba1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after dropping rows with ANY NaN:\n",
      "      A    B   C\n",
      "0  1.0  5.0   9\n",
      "3  4.0  8.0  12\n"
     ]
    }
   ],
   "source": [
    "# Drop any row that contains at least one NaN value\n",
    "df_dropped_rows = df_nan.dropna(axis=0, how='any')\n",
    "print(\"\\nDataFrame after dropping rows with ANY NaN:\\n\", df_dropped_rows)\n",
    "\n",
    "# Drop columns where ALL values are NaN (not applicable here, but common)\n",
    "# df_dropped_cols = df_nan.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5d21d-b3b1-4420-bd23-e4ebc5df2a2d",
   "metadata": {},
   "source": [
    "### 6.3 Filling Missing Data (.fillna())\n",
    "Instead of dropping data, you can fill the missing values with a replacement value (imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81813a2f-2437-40d1-a9c1-c8707b74602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after filling NaN with 0:\n",
      "      A    B   C\n",
      "0  1.0  5.0   9\n",
      "1  2.0  0.0  10\n",
      "2  0.0  7.0  11\n",
      "3  4.0  8.0  12\n",
      "\n",
      "Mean of A (2.33) used for imputation:\n",
      "           A    B   C\n",
      "0  1.000000  5.0   9\n",
      "1  2.000000  NaN  10\n",
      "2  2.333333  7.0  11\n",
      "3  4.000000  8.0  12\n"
     ]
    }
   ],
   "source": [
    "# Fill all NaN values with a single fixed value (e.g., 0)\n",
    "df_filled_zero = df_nan.fillna(0)\n",
    "print(\"\\nDataFrame after filling NaN with 0:\\n\", df_filled_zero)\n",
    "\n",
    "# Fill NaN with the MEAN of that column (common imputation method)\n",
    "mean_a = df_nan['A'].mean()\n",
    "df_filled_mean = df_nan.fillna({'A': mean_a}) # Fill only column 'A' with its mean\n",
    "print(f\"\\nMean of A ({mean_a:.2f}) used for imputation:\\n\", df_filled_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad91ebb6-4342-4268-afa3-2b2df8105987",
   "metadata": {},
   "source": [
    "## 7. Merging and Joining DataFrames (The \"SQL JOIN\" of Pandas)\n",
    "   In real-world analysis, data often lives in multiple tables. Merging is how you combine DataFrames based on a shared column (a key), much like a JOIN operation in SQL.The primary function is **pd.merge()**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36097a50-bc2d-4239-b728-041b216322b3",
   "metadata": {},
   "source": [
    "#### Key Arguments of pd.merge()\n",
    "**left** / **right**: The two DataFrames to merge.\n",
    "\n",
    "**on**: The column name(s) to join on (the common key).\n",
    "\n",
    "**how**: Specifies the type of join:\n",
    "    \n",
    "**'inner'** (Default): Returns only rows that have matching keys in both DataFrames.\n",
    "    \n",
    "**'left'**: Returns all rows from the left DataFrame, and the matching rows from the right. **NaN** is filled where there is no match.\n",
    "    \n",
    "**'right'**: Returns all rows from the right DataFrame, and the matching rows from the left.\n",
    "    \n",
    "**'outer'**: Returns all rows when there is a match in either the left or the right DataFrame (full union)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaf0088e-f650-48df-a507-2f4004dd7a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Merge Result:\n",
      "   Emp_ID     Name Project\n",
      "0       1    Alice   Alpha\n",
      "1       2      Bob    Beta\n",
      "2       3  Charlie   Gamma\n"
     ]
    }
   ],
   "source": [
    "# DataFrame 1: Employee Names and IDs\n",
    "df_employees = pd.DataFrame({'Emp_ID': [1, 2, 3, 4], \n",
    "                             'Name': ['Alice', 'Bob', 'Charlie', 'David']})\n",
    "\n",
    "# DataFrame 2: Project Assignments and IDs (Missing Emp_ID 4)\n",
    "df_projects = pd.DataFrame({'Emp_ID': [1, 2, 3, 5], \n",
    "                            'Project': ['Alpha', 'Beta', 'Gamma', 'Zeta']})\n",
    "\n",
    "# Inner Merge: Only includes employees found in BOTH tables (1, 2, 3)\n",
    "merged_inner = pd.merge(df_employees, df_projects, on='Emp_ID', how='inner')\n",
    "\n",
    "print(\"Inner Merge Result:\")\n",
    "print(merged_inner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e05b0d0-8f66-404f-9643-2d10d860bec1",
   "metadata": {},
   "source": [
    "## 8. Applying Custom Functions ({.apply() and .map())\n",
    "   While vectorized operations (like **df['A'] + df['B']**) are fastest, sometimes you need to run complex, custom logic that Python functions are better suited for.\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819a66d-d413-4487-baca-898e54821068",
   "metadata": {},
   "source": [
    "#### **.apply()**\n",
    "   \n",
    "   The **.apply()** method is used to apply a function along an axis of a DataFrame (row or column) or to a specific **Series**. It's great for complex column transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "598d48ec-85ce-47ee-bcc8-dc2b06edce23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying Custom Function (.apply()):\n",
      "   Score Grade\n",
      "0     85     B\n",
      "1     92     A\n",
      "2     78     C\n",
      "3     95     A\n",
      "4     60     F\n"
     ]
    }
   ],
   "source": [
    "# Create a simple DataFrame\n",
    "df_scores = pd.DataFrame({'Score': [85, 92, 78, 95, 60]})\n",
    "\n",
    "# Function to assign a letter grade\n",
    "def get_grade(score):\n",
    "    if score >= 90:\n",
    "        return 'A'\n",
    "    elif score >= 80:\n",
    "        return 'B'\n",
    "    elif score >= 70:\n",
    "        return 'C'\n",
    "    else:\n",
    "        return 'F'\n",
    "\n",
    "# Apply the function to the 'Score' column (which is a Series)\n",
    "df_scores['Grade'] = df_scores['Score'].apply(get_grade)\n",
    "\n",
    "print(\"\\nApplying Custom Function (.apply()):\")\n",
    "print(df_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecddf12-7783-4c7f-b841-e0b1e1bcf421",
   "metadata": {},
   "source": [
    "#### **.map()** (For Series only)\n",
    "\n",
    "The **.map()** method is specialized for a Series and is ideal for substituting values based on a dictionary or another Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b53816c-b0a0-4209-ba72-6ba734d56463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying Dictionary Mapping (.map()):\n",
      "   Score Grade Pass_Fail\n",
      "0     85     B      Pass\n",
      "1     92     A      Pass\n",
      "2     78     C      Pass\n",
      "3     95     A      Pass\n",
      "4     60     F      Fail\n"
     ]
    }
   ],
   "source": [
    "df_scores['Pass_Fail'] = df_scores['Grade'].map({'A': 'Pass', 'B': 'Pass', 'C': 'Pass', 'F': 'Fail'})\n",
    "print(\"\\nApplying Dictionary Mapping (.map()):\")\n",
    "print(df_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e3790e-c002-49e6-8ea1-5464ab59f3d4",
   "metadata": {},
   "source": [
    "## 9. Pivot Tables and .crosstab() (Reshaping for Summaries)\n",
    "  These tools are essential for restructuring and summarizing data, often to make it resemble a traditional financial or statistical report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d5d61-ed06-419c-af42-c64e080a6b26",
   "metadata": {},
   "source": [
    "#### .pivot_table()\n",
    "\n",
    "This is used to create a spreadsheet-style pivot table as a DataFrame. It's similar to **.groupby()**, but allows you to place one categorical variable on the rows and another on the columns.\n",
    "\n",
    "**index**: The column(s) to become the row index.\n",
    "\n",
    "**columns**: The column(s) to become the new column headers.\n",
    "\n",
    "**values**: The column(s) to aggregate.\n",
    "\n",
    "**aggfunc**: The aggregation function (sum, mean, count, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4f6b030-217a-436b-a364-cf55baa0e843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pivot Table (Region vs. Product Sales):\n",
      "Product    A    B\n",
      "Region           \n",
      "East     230  150\n",
      "West     120  200\n"
     ]
    }
   ],
   "source": [
    "# Sample data with Region, Product, and Sales\n",
    "df_data = pd.DataFrame({\n",
    "    'Region': ['East', 'East', 'West', 'West', 'East'],\n",
    "    'Product': ['A', 'B', 'A', 'B', 'A'],\n",
    "    'Sales': [100, 150, 120, 200, 130]\n",
    "})\n",
    "\n",
    "# Pivot: Summarise total Sales by Region AND Product\n",
    "sales_pivot = df_data.pivot_table(\n",
    "    index='Region', \n",
    "    columns='Product', \n",
    "    values='Sales', \n",
    "    aggfunc='sum'\n",
    ")\n",
    "\n",
    "print(\"\\nPivot Table (Region vs. Product Sales):\")\n",
    "print(sales_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070b5de-9f15-47fc-8cfd-9e03639c0a91",
   "metadata": {},
   "source": [
    "#### pd.crosstab() (Frequency Tables)\n",
    "A special case of a pivot table used to compute a simple frequency table of two or more categorical factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7867d32-6c57-4565-a22f-78c527f9b25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crosstab (Frequency Table):\n",
      "Product  A  B\n",
      "Region       \n",
      "East     2  1\n",
      "West     1  1\n"
     ]
    }
   ],
   "source": [
    "# Count how many times each Product appears in each Region\n",
    "frequency_table = pd.crosstab(df_data['Region'], df_data['Product'])\n",
    "print(\"\\nCrosstab (Frequency Table):\")\n",
    "print(frequency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97685782-8130-483a-994f-d6a30d20493c",
   "metadata": {},
   "source": [
    "## 10.MultiIndex (Hierarchical Indexing) \n",
    "A MultiIndex allows a Pandas object to have multiple levels of indexing on an axis (rows or columns), which is often the result of using \n",
    "**.groupby()** on multiple keys or using **.pivot\\_table()**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d95628-f8b3-4a81-9959-82b8c6c9a3ca",
   "metadata": {},
   "source": [
    "Example: Multi-Level Grouping\n",
    "\n",
    "When you group by two columns, the result naturally creates a MultiIndex:Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bccbc039-0c84-448d-be0f-deca177df5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Series with MultiIndex (Region, Product):\n",
      "Region  Product\n",
      "East    A          115.0\n",
      "        B          150.0\n",
      "West    A          120.0\n",
      "        B          200.0\n",
      "Name: Sales, dtype: float64\n",
      "Index Type: MultiIndex([('East', 'A'),\n",
      "            ('East', 'B'),\n",
      "            ('West', 'A'),\n",
      "            ('West', 'B')],\n",
      "           names=['Region', 'Product'])\n"
     ]
    }
   ],
   "source": [
    "# Group by both Region AND Product, and calculate the mean Sales\n",
    "multi_group = df_data.groupby(['Region', 'Product'])['Sales'].mean()\n",
    "\n",
    "print(\"\\nSeries with MultiIndex (Region, Product):\")\n",
    "print(multi_group)\n",
    "print(f\"Index Type: {multi_group.index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc190c-c3ad-4f91-8a7f-06b38a5d5274",
   "metadata": {},
   "source": [
    "#### Key Point: \n",
    "You access data in a MultiIndex using tuples for indexing (e.g., **multi _group.loc[('East', 'A')]**  will give you the sales for Product A in the East region)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ce7fd59-23d5-47aa-b4bc-c5d10f96b6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(115.0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_group.loc[('East','A')] # using .loc() you can get values by index names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
